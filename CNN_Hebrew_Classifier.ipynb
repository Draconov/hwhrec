{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3295a946",
   "metadata": {},
   "source": [
    "# ðŸ§  CNN Classifier for Handwritten Hebrew Letters\n",
    "This notebook implements a deep CNN to classify handwritten Hebrew letters using the HHD dataset.\n",
    "\n",
    "**Experiments:**\n",
    "- Training without data augmentation\n",
    "- Training with data augmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247b5ec4",
   "metadata": {},
   "source": [
    "## ðŸ“¦ 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0fd4e0",
   "metadata": {},
   "source": [
    "# ðŸ§  CNN Classifier for Handwritten Hebrew Letters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2945e16",
   "metadata": {},
   "source": [
    "## ðŸ§¼ 2. Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e355f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5d8c49",
   "metadata": {},
   "source": [
    "## ðŸ“¥ 3. Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e2438c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        return None\n",
    "    h, w = img.shape\n",
    "    if h > w:\n",
    "        pad = (h - w) // 2\n",
    "        img = cv2.copyMakeBorder(img, 0, 0, pad, h - w - pad, cv2.BORDER_CONSTANT, value=255)\n",
    "    elif w > h:\n",
    "        pad = (w - h) // 2\n",
    "        img = cv2.copyMakeBorder(img, pad, w - h - pad, 0, 0, cv2.BORDER_CONSTANT, value=255)\n",
    "    img = cv2.resize(img, (32, 32))\n",
    "    img = 255 - img\n",
    "    return img.astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d9afbd",
   "metadata": {},
   "source": [
    "## ðŸ”€ 4. Load and Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda705e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(base_dir):\n",
    "    X, y = [], []\n",
    "    for label in range(27):\n",
    "        folder = os.path.join(base_dir, str(label))\n",
    "        if not os.path.exists(folder): continue\n",
    "        for fname in os.listdir(folder):\n",
    "            img_path = os.path.join(folder, fname)\n",
    "            img = preprocess_image(img_path)\n",
    "            if img is not None:\n",
    "                X.append(img.reshape(32, 32, 1))\n",
    "                y.append(label)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23bfafc",
   "metadata": {},
   "source": [
    "## ðŸ§  5. CNN Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc918cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_dataset('processed_hhd/train')\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, stratify=y, random_state=42)\n",
    "X_test, y_test = load_dataset('processed_hhd/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b2b620",
   "metadata": {},
   "source": [
    "## ðŸš‚ 6. Train CNN (No Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8f35d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn():\n",
    "    model = Sequential()\n",
    "    for filters in [32, 64, 128]:\n",
    "        model.add(Conv2D(filters, (3, 3), activation='relu', padding='same'))\n",
    "        model.add(Conv2D(filters, (3, 3), activation='relu', padding='same'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(27, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9001d993",
   "metadata": {},
   "source": [
    "## ðŸš€ 7. Train CNN (With Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c251aa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_cnn()\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089d57a2",
   "metadata": {},
   "source": [
    "## ðŸ“‰ 8. Plot Training vs Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50de2f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    rotation_range=10,\n",
    "    shear_range=0.2,\n",
    "    brightness_range=(0.2, 1.8))\n",
    "datagen.fit(X_train)\n",
    "model_aug = build_cnn()\n",
    "history_aug = model_aug.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
    "                             validation_data=(X_val, y_val), epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7003bbe3",
   "metadata": {},
   "source": [
    "## ðŸ“Š 9. Evaluate on Test Set and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2acccb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='Train Loss (no aug)')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss (no aug)')\n",
    "plt.plot(history_aug.history['loss'], label='Train Loss (aug)')\n",
    "plt.plot(history_aug.history['val_loss'], label='Val Loss (aug)')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Training and Validation Loss')\n",
    "plt.legend(); plt.grid(True); plt.savefig('loss_curve.png'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cf9082",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model_aug.predict(X_test), axis=1)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "acc_per_class = cm.diagonal() / cm.sum(axis=1)\n",
    "for i, acc in enumerate(acc_per_class):\n",
    "    print(f'Letter {i}: {acc:.2f}')\n",
    "print('Average accuracy:', acc_per_class.mean())\n",
    "pd.DataFrame(cm).to_csv('confusion_matrix.csv', index=False)\n",
    "with open('results.txt', 'w') as f:\n",
    "    f.write('Letter    Accuracy\\n')\n",
    "    for i, acc in enumerate(acc_per_class):\n",
    "        f.write(f'{i:<10}{acc:.4f}\\n')\n",
    "    f.write(f'\\nAverage accuracy: {acc_per_class.mean():.4f}\\n')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
